{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Data that we use are from the following links:\n",
    "\n",
    "1) [Fake and real news dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset). The data sets we use are:\n",
    "\n",
    "    * Fake.csv.zip\n",
    "    * True.csv.zip\n",
    "    \n",
    "2) [Source based Fake News Classification](https://www.kaggle.com/datasets/ruchi798/source-based-news-classification?select=news_articles.csv). The data sets we use are:\n",
    "\n",
    "    * news_articles.csv.zip\n",
    "    \n",
    "    \n",
    "3) [REAL and FAKE news dataset](https://www.kaggle.com/datasets/nopdev/real-and-fake-news-dataset?select=news.csv). The data sets we use are:\n",
    "\n",
    "    * news.csv\n",
    "    \n",
    "    \n",
    "4) [GitHub Repo](https://github.com/KaiDMML/FakeNewsNet). The data sets we use are:\n",
    "\n",
    "    * politifact_fake.csv\n",
    "    * politifact_real.csv\n",
    "    * gossipcop_fake.csv\n",
    "    * gossipcop_real.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Libraries that we will use are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/miladshirani/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')#,parse=True,tag=True, entity=True)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    " \n",
    "import unicodedata             ## to remove accented and special chracters\n",
    "from textblob import TextBlob  ## to calculate Polarity and Subjectivity (Sentiment)\n",
    "# pip install textblob\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions We Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will define several functions to extract web addresses, phone numbers, date, time and any digits in the following. At the end, we will have a function to perform all the work at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Dropping text with no words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function takes the data, and will drop the rows where specific column has less than a minimum number of tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def empty_text(data, column, min_num_word):\n",
    "    data[\"number_of_word\"] = data[column].str.split().str.len()\n",
    "\n",
    "    index = data.loc[data[\"number_of_word\"] < min_num_word].index\n",
    "    data  = data.drop(index = index, axis = 0)\n",
    "    data  = data.drop(\"number_of_word\", axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Feature extraction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function is compilation of all the other cleaning functions which will be introduced later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_it(data, keyword, replace):\n",
    "    \n",
    "        \n",
    "    if keyword == \"link\":\n",
    "        return link_finder(data, replace)\n",
    "    elif keyword == \"id\":\n",
    "        return id_finder(data, replace)\n",
    "    elif keyword == \"char\":\n",
    "        return char_finder(data, replace)\n",
    "    elif keyword == \"digit\":\n",
    "        return all_digit_finder(data, replace)\n",
    "    elif keyword == \"acc\":\n",
    "        return remove_accented(data)\n",
    "    elif keyword == \"email\":\n",
    "        return email_finder(data, replace)\n",
    "    elif keyword == \"Reuters\":\n",
    "        return reuters_finder(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Finding Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function will find links in a text, calculate the number of links and will replace them with a white space. The pattern used in the following function is from [here](https://stackoverflow.com/questions/6038061/regular-expression-to-find-urls-within-a-string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def link_finder(data, replace):\n",
    "    pattern = '(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+'\n",
    "    \n",
    "    if replace == False:\n",
    "        return len(re.findall(pattern, data))\n",
    "    else:\n",
    "        return re.sub(pattern,\" \",data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello   World   \\nand   and  \\nalso this one @DanPatrick.    Shannon Watts (@shannonrwatts) \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the function\n",
    "\n",
    "link_finder_data = \"\"\"Hello www.google.com World http://yahoo.com \n",
    "and www.berkeley.edu and https://s123rd.edu/12332100kashh123jsa/dhsas?dsajsd\n",
    "also this one @DanPatrick. pic.twitter.com/mUbKCIWGxB  Shannon Watts (@shannonrwatts) \n",
    "\"\"\"\n",
    "\n",
    "link_finder(link_finder_data, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Finding IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function will find and replace the ids in a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def id_finder(data, replace):\n",
    "    pattern = '@[a-zA-Z0-9]+'\n",
    "    \n",
    "    if replace == False:\n",
    "        return len(re.findall(pattern, data))\n",
    "    else:\n",
    "        return re.sub(pattern,\" \",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello   World   \\nand   and  \\nalso this one  .    Shannon Watts ( ) \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the function\n",
    "\n",
    "link_finder_data = \"\"\"Hello www.google.com World http://yahoo.com \n",
    "and www.berkeley.edu and https://s123rd.edu/12332100kashh123jsa/dhsas?dsajsd\n",
    "also this one @DanPatrick. pic.twitter.com/mUbKCIWGxB  Shannon Watts (@shannonrwatts) \n",
    "\"\"\"\n",
    "\n",
    "no_id = id_finder(link_finder_data, True)\n",
    "\n",
    "link_finder(no_id, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Finding characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function will find any non alphabetic and numeric characters in a text and will replace them with a whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def char_finder(data, replace):\n",
    "    pattern = r'[^a-zA-z0-9]'\n",
    "    if replace == False:\n",
    "        return len(re.findall(pattern, data))\n",
    "    else: \n",
    "        return re.sub(pattern,\" \",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  DanPatrick  pic twitter com mUbKCIWGxB  Shannon Watts   shannonrwatts   '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = \"\"\"\n",
    "@DanPatrick. pic.twitter.com/mUbKCIWGxB  Shannon Watts (@shannonrwatts) \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "char_finder(test_data, replace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Finding accented characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function will replace any accented characters with an english alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_accented(data):\n",
    "    new_data = unicodedata.normalize('NFKD', data)\\\n",
    "                          .encode('ascii', 'ignore')\\\n",
    "                          .decode('utf-8', 'ignore')\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Finding numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function will find any numeric data such as numbers, phone numbers and dates and will replace them with whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def all_digit_finder(data, replace):\n",
    "    \"\"\"\n",
    "    the pattern \n",
    "    \n",
    "    \"\\+?\\s?\\d+.?\\d+.?\\d+.?\\d+|\\(\\d+\\) \\d+.?\\d+\"\n",
    "    \n",
    "    can be used to get the digits, dates and phone numbers\n",
    "    of the form\n",
    "    \n",
    "    12.2345  \n",
    "    0.0123   \n",
    "    9876  \n",
    "    \n",
    "    dates of the form:\n",
    "    \n",
    "    12/21/2020  \n",
    "    2020-01-22  \n",
    "    2020/02/23\n",
    "    \n",
    "    phone numbers of the form:\n",
    "    \n",
    "    (911) 820 2230\n",
    "    (911) 820-223\n",
    "    911-820-2230   \n",
    "    +1-814-929-2533  \n",
    "    0018149292533\n",
    "    \n",
    "    time of the form: \n",
    "    \n",
    "    12:00\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    To get phone numbers of the form \n",
    "    (XXX) XXX-XXX or XXX-XXX-XXXX or +X-XXX-XXX-XXXX\n",
    "    \n",
    "    we can use the following pattern\n",
    "    \"\\+?\\d+.\\d+.\\d+.\\d+.\\d+|\\(\\d+\\) \\d+.\\d+\"\n",
    "    \n",
    "    to get numbers of the form:\n",
    "    XXX or XX.XXX\n",
    "    we can use the following pattern\n",
    "    \"(\\s\\d+\\s|\\s\\d+\\.\\d+\\s)\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    pattern = \"\\d+|\\s\\+?\\s?\\d+.?\\d+.?\\d+.?\\d+|\\(\\d+\\) \\d+.?\\d+|\\s+\\d+\" \n",
    "\n",
    "#     pattern = \"\\+?\\d+.\\d+.\\d+.\\d+.\\d+|\\(\\d+\\) \\d+.\\d+\"\n",
    "    \n",
    "    if replace == False: \n",
    "        return len(re.findall(pattern, data))\n",
    "    else:\n",
    "        return re.sub(pattern, \" \", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  December , The\n"
     ]
    }
   ],
   "source": [
    "date_finder_data = \"\"\"3\n",
    "\n",
    "Hello https://s123rd.edu/12332100kashh123jsa/dhsas?dsajsd\n",
    "and 1st digit: 12.2345 or 2nd digit: 0.0123 or 3rd digit: 9876 \n",
    "\n",
    "and 1st date: 12/21/2020 or 2nd date: 2020-01-22 or 3rd date: 2020/02/23 \n",
    "\n",
    "call 1st number: (911) 820-2230 or 2nd number: 911-820-2230 or \n",
    "3rd number: (911) 820 2230 or call 4th number: +1-814-929-2533\n",
    "\n",
    "or call 5th number: 0018149292533 at 1st time: 12:00\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test_2 = \"4 December 31, 2017The\"\n",
    "\n",
    "print(all_digit_finder(test_2, replace = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Email Address Finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function will find any email address in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def email_finder(data, replace):\n",
    "    \n",
    "    pattern = \"[a-zA-Z0-9]+.?[a-zA-Z0-9]+@[a-zA-Z0-9]+\\.[a-z]+\"\n",
    "    \n",
    "    if replace == False:\n",
    "        return len(re.findall(pattern, data))\n",
    "    \n",
    "    else:\n",
    "        return re.sub(pattern, \"emailaddress\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nplease email emailaddress or emailaddress\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = \"\"\"\n",
    "\n",
    "please email ab23_bts@gmail.com or 123asB.asd@as12.com\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "email_finder(email, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Removing Reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the text of the data, we realized that there are a lot of text that are reported by the Reuters and we decided to remove this word from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reuters_finder(data):\n",
    "    \n",
    "    pattern = \"Reuters\"\n",
    "    \n",
    "    return re.sub(pattern, \" \", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WASHINGTON ( ) said'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = \"WASHINGTON (Reuters) said\"\n",
    "\n",
    "\n",
    "reuters_finder(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Cleaning Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function will clean the data, tokenize and lemmatize the data. It uses the \"find_it\" and \"tokenizing_lemmatizing\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cleaning(data):\n",
    "    pre_cleaning_list = [ \"Reuters\", \"email\", \"id\", \"link\", \"digit\", \"char\", \"acc\"]\n",
    "    \n",
    "    for item in pre_cleaning_list:\n",
    "        data = find_it(data, item, replace = True)\n",
    "        \n",
    "    tokenized_data = tokenizing_lemmatizing(data)\n",
    "    \n",
    "    return tokenized_data\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tokenizing and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function takes the cleaned text and will tokenize and stem the text and if the token is in the stopwords, it will remove them from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenizing_stemming(data):\n",
    "    \n",
    "    stemmer = SnowballStemmer(language=\"english\")\n",
    "    basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "    tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "    token_list = tokenizer.tokenize(data)\n",
    "    return [stemmer.stem(token.lower()) for token in token_list if token.lower() not in stop_words]\n",
    "#     return [token.lower() for token in token_list if token.lower() not in stop_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tokenizing and Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function takes the cleaned text and will tokenize and lemmatize the text and if the token is in the stopwords, it will remove them from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenizing_lemmatizing(data):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "    tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "    token_list = tokenizer.tokenize(data)\n",
    "#     return [stemmer.stem(token.lower()) for token in token_list if token.lower() not in stop_words]\n",
    "    return [lemmatizer.lemmatize(token.lower(), pos = \"v\") for token in token_list \n",
    "            if token.lower() not in stop_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## For GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This function will clean the text and will return a cleaned text while it has the stopwords because we are going to use it in the notebook, \"Modeling_GloVe\" in the modeling section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def for_glove(data):\n",
    "    p_link = '(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+'\n",
    "    p_id = '@[a-zA-Z0-9]+'\n",
    "    p_char = r'[^a-zA-z0-9]'\n",
    "    p_num = \"\\d+|\\s\\+?\\s?\\d+.?\\d+.?\\d+.?\\d+|\\(\\d+\\) \\d+.?\\d+|\\s+\\d+\"\n",
    "    p_email = \"[a-zA-Z0-9]+.?[a-zA-Z0-9]+@[a-zA-Z0-9]+\\.[a-z]+\"\n",
    "    p_reut = \"[a-zA-Z]+\\s+reuters\\s+\"\n",
    "\n",
    "    d_email   = re.sub(p_email, \" \", data)\n",
    "    d_id      = re.sub(p_id   , \" \", d_email)\n",
    "    d_link    = re.sub(p_link, \" \", d_id)\n",
    "    d_num     = re.sub(p_num, \" \", d_link)\n",
    "    d_char    = re.sub(p_char, \" \", d_num)\n",
    "    d_reuters = re.sub(p_reut, \" \", d_char)\n",
    "    \n",
    "    cleaned_data = unicodedata.normalize('NFKD', d_reuters)\\\n",
    "                                 .encode('ascii', 'ignore')\\\n",
    "                                 .decode('utf-8', 'ignore')\n",
    "    \n",
    "    return word_tokenize(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this subsection we want to check and test the functions we introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', 'and', 'and', 'also', 'this', 'one', 'Shannon', 'Watts', 'would', 'could', 'might', 'Shannon', 'Watts', 'Hello', 'and', 'st', 'digit', 'or', 'nd', 'digit', 'or', 'rd', 'digit', 'and', 'st', 'date', 'or', 'nd', 'date', 'or', 'rd', 'date', 'call', 'st', 'number', 'or', 'nd', 'number', 'or', 'rd', 'number', 'or', 'call', 'th', 'number', 'or', 'call', 'th', 'Number', 'at', 'st', 'time', 'please', 'email', 'or']\n"
     ]
    }
   ],
   "source": [
    "to_test = \"\"\"2 @Flatiron\n",
    "\n",
    "Hello www.google.com World http://yahoo.com \n",
    "and www.berkeley.edu and https://s123rd.edu/12332100kashh123jsa/dhsas?dsajsd\n",
    "also this one @DanPatrick. pic.twitter.com/mUbKCIWGxB  Shannon Watts (@shannonrwatts) \n",
    "\n",
    "would could might\n",
    "\n",
    "@DanPatrick. pic.twitter.com/mUbKCIWGxB  Shannon Watts (@shannonrwatts) \n",
    "\n",
    "\n",
    "Hello https://s123rd.edu/12332100kashh123jsa/dhsas?dsajsd\n",
    "and 1st digit: 12.2345 or 2nd digit: 0.0123 or 3rd digit: 9876 \n",
    "\n",
    "and 1st date: 12/21/2020 or 2nd date: 2020-01-22 or 3rd date: 2020/02/23 \n",
    "\n",
    "call 1st number: (911) 820-2230 or 2nd number: 911-820-2230 or \n",
    "3rd number: (911) 820 2230 or call 4th number: +1-814-929-2533\n",
    "\n",
    "or call 5th Number: 0018149292533 at 1st time: 12:00\n",
    "\n",
    "please email ab23_bts@gmail.com or 123asB.asd@as12.com\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# print(cleaning(to_test))\n",
    "print(for_glove(to_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing Data and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this part we import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path = \"../EDA/Raw_Data/\"\n",
    "## DataSet 1\n",
    "fake = pd.read_csv(path + \"Fake.csv\")\n",
    "true = pd.read_csv(path + \"True.csv\")\n",
    "\n",
    "## DataSet 2\n",
    "g_fake = pd.read_csv(path + \"gossipcop_fake.csv\")\n",
    "g_real = pd.read_csv(path + \"gossipcop_real.csv\")\n",
    "p_fake = pd.read_csv(path + \"politifact_fake.csv\")\n",
    "p_real = pd.read_csv(path + \"politifact_real.csv\")\n",
    "\n",
    "## DataSet 3\n",
    "articles = pd.read_csv(path + \"news_articles.csv\")\n",
    "\n",
    "\n",
    "## DataSet 4\n",
    "news = pd.read_csv(path + \"news-II.csv\")\n",
    "\n",
    "\n",
    "real_fake = {\"Real\" : \"True\", \"REAL\": \"True\" , 1:\"True\",\n",
    "             \"FAKE\": \"Fake\", \"Fake\": \"Fake\", 0:\"Fake\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Cleaning of Each DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, we clean the text data in each dataframe separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Fake_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We first create a column in each dataframe called \"label\" to assign a label to the data, then we concatenate them to each other. After that, we drop the columns `title`, `date` and  `subject`. Since `fake_true` dataframe is relatively big, we divide it into 12 dataframes and then we do the cleaning process to each dataframe. After cleaning each dataframe, we concatenate them again to make a one cleaned dataframe and then we save it into the \"cleaned\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44898"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake[\"label\"] = \"Fake\"\n",
    "true[\"label\"] = \"True\"\n",
    "\n",
    "fake_true = pd.concat([fake, true], axis = 0)\n",
    "fake_true.drop(['title', \"date\", \"subject\"], inplace = True, axis = 1)\n",
    "\n",
    "fake = empty_text(fake, \"text\", 5)\n",
    "true = empty_text(true, \"text\", 5)\n",
    "\n",
    "len(fake_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ft_1  = fake_true.iloc[:4000, :].copy()\n",
    "ft_2  = fake_true.iloc[4000:8000, :].copy()\n",
    "ft_3  = fake_true.iloc[8000:12000, :].copy()\n",
    "ft_4  = fake_true.iloc[12000:16000, :].copy()\n",
    "ft_5  = fake_true.iloc[16000:20000, :].copy()\n",
    "ft_6  = fake_true.iloc[20000:24000, :].copy()\n",
    "ft_7  = fake_true.iloc[24000:28000, :].copy()\n",
    "ft_8  = fake_true.iloc[28000:32000, :].copy()\n",
    "ft_9  = fake_true.iloc[32000:34000, :].copy()\n",
    "ft_10 = fake_true.iloc[34000:36000, :].copy()\n",
    "ft_11 = fake_true.iloc[36000:40000, :].copy()\n",
    "ft_12 = fake_true.iloc[40000:, :].copy()\n",
    "\n",
    "list_of_df = [ft_1, ft_2, ft_3, ft_4, ft_5, ft_6, ft_7,\n",
    "             ft_8, ft_9, ft_10, ft_11, ft_12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "2\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "3\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "4\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "5\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "6\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "7\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "8\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "9\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "10\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "11\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n",
      "12\n",
      "\tCleaning Done\n",
      "\tFor GloVe Done\n",
      "\tLink Done\n",
      "\tSentiment Done\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_values = ['neg', 'neu', 'pos', 'compound']\n",
    "\n",
    "for i, item in enumerate(list_of_df):\n",
    "    print(i+1)\n",
    "    \n",
    "    item[\"cleaned\"]  = item[\"text\"].apply(lambda x: cleaning(x.lower()))\n",
    "    print(\"\\tCleaning Done\")\n",
    "    \n",
    "    item[\"for_glove\"]  = item[\"text\"].apply(lambda x: for_glove(x))\n",
    "    print(\"\\tFor GloVe Done\")\n",
    "    \n",
    "    item[\"num_urls\"] = item[\"text\"].apply(lambda x: link_finder(x, False))\n",
    "    print(\"\\tLink Done\")\n",
    "    \n",
    "    item[sentiment_values] = item[\"text\"].apply(sid.polarity_scores).apply(pd.Series)\n",
    "    print(\"\\tSentiment Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to Directory\n"
     ]
    }
   ],
   "source": [
    "fake_true_cleaned = ft_1.copy()\n",
    "to_concat = [ft_2, ft_3, ft_4, ft_5, ft_6, ft_7,\n",
    "             ft_8, ft_9, ft_10, ft_11, ft_12]\n",
    "for item in to_concat:\n",
    "    fake_true_cleaned = pd.concat([fake_true_cleaned, item], axis = 0)\n",
    "fake_true_cleaned.to_csv(\"../EDA/cleaned/fake_true.csv\")   \n",
    "print(\"Saved to Directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## gossip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We first create a column in each dataframe called \"label\" to assign a label to the data, then we concatenate them to each other. After that, we drop the columns `id`, `news_url`, and `tweet_ids`. Since `gossip` dataframe is not relatively big, we do the cleaning process to it without dividing it to different dataframes. After cleaning each dataframe, we concatenate them again to make a one cleaned dataframe and then we save it into the \"cleaned\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_fake[\"label\"] = \"Fake\"\n",
    "g_real[\"label\"] = \"True\"\n",
    "p_fake[\"label\"] = \"Fake\"\n",
    "p_real[\"label\"] = \"True\"\n",
    "\n",
    "gossip = pd.concat([g_fake,g_real, p_fake, p_real], axis = 0)\n",
    "gossip.drop([\"id\", \"news_url\", 'tweet_ids'], inplace = True, axis = 1)\n",
    "gossip.rename(columns = {\"title\": \"text\"}, inplace = True)\n",
    "\n",
    "\n",
    "gossip = empty_text(gossip, \"text\",5)\n",
    "\n",
    "gossip.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Done\n",
      "For GloVe Done\n",
      "Link Done\n",
      "Sentiment Done\n",
      "Saved to Directory\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_values = ['neg', 'neu', 'pos', 'compound']\n",
    "\n",
    "\n",
    "gossip[\"cleaned\"] = gossip[\"text\"].apply(lambda x: cleaning(x))\n",
    "print(\"Cleaning Done\")\n",
    "\n",
    "gossip[\"for_glove\"]  = gossip[\"text\"].apply(lambda x: for_glove(x))\n",
    "print(\"For GloVe Done\")\n",
    "\n",
    "gossip[\"num_urls\"]= gossip[\"text\"].apply(lambda x: link_finder(x, False))\n",
    "print(\"Link Done\")\n",
    "\n",
    "gossip[sentiment_values] = gossip[\"text\"].apply(sid.polarity_scores).apply(pd.Series)\n",
    "print(\"Sentiment Done\")\n",
    "\n",
    "gossip.to_csv(\"../EDA/cleaned/gossip.csv\")\n",
    "print(\"Saved to Directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## articles_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We first map the values in the column `lable` to `Fake` and `True`, then we drop several columns. Since `article_en` dataframe is not relatively big, we do the cleaning process to it without dividing it to different dataframes. After cleaning each dataframe, we concatenate them again to make a one cleaned dataframe and then we save it into the \"cleaned\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1943"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[\"label\"] = articles[\"label\"].map(real_fake)\n",
    "\n",
    "to_drop = ['author', 'published','title','site_url','main_img_url', 'type',\n",
    "           'title_without_stopwords','text_without_stopwords', 'hasImage']\n",
    "\n",
    "articles.drop(columns = to_drop, inplace = True, axis = 1)\n",
    "\n",
    "articles_en = articles[articles[\"language\"] == \"english\"][[\"text\", \"label\"]]\n",
    "\n",
    "articles_en.isna().sum()\n",
    "articles_en.dropna(inplace = True, axis = 0)\n",
    "articles_en.isna().sum()\n",
    "articles_en = empty_text(articles_en, \"text\", 5)\n",
    "len(articles_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Done\n",
      "For GloVe Done\n",
      "Link Done\n",
      "Sentiment Done\n",
      "Saved to Directory\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_values = ['neg', 'neu', 'pos', 'compound']\n",
    "\n",
    "articles_en[\"cleaned\"] = articles_en[\"text\"].apply(lambda x: cleaning(x))\n",
    "print(\"Cleaning Done\")\n",
    "\n",
    "articles_en[\"for_glove\"]  = articles_en[\"text\"].apply(lambda x: for_glove(x))\n",
    "print(\"For GloVe Done\")\n",
    "\n",
    "articles_en[\"num_urls\"]= articles_en[\"text\"].apply(lambda x: link_finder(x, False))\n",
    "print(\"Link Done\")\n",
    "\n",
    "articles_en[sentiment_values] = articles_en[\"text\"]\\\n",
    ".apply(sid.polarity_scores).apply(pd.Series)\n",
    "print(\"Sentiment Done\")\n",
    "articles_en.to_csv(\"../EDA/cleaned/articles_en.csv\")\n",
    "print(\"Saved to Directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We first map the values in the column `lable` to `Fake` and `True`, then we drop columns `Unnamed: 0`, `title`. Since `gossip` dataframe is not relatively big, we do the cleaning process to it without dividing it to different dataframes. After cleaning each dataframe, we concatenate them again to make a one cleaned dataframe and then we save it into the \"cleaned\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "news[\"label\"] = news[\"label\"].map(real_fake)\n",
    "news.drop(['Unnamed: 0', 'title'], axis = 1, inplace = True)\n",
    "\n",
    "news = empty_text(news, \"text\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Done\n",
      "For GloVe Done\n",
      "Cleaning Done\n",
      "Sentiment Done\n",
      "Saved to Directory\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_values = ['neg', 'neu', 'pos', 'compound']\n",
    "\n",
    "news[\"cleaned\"] = news[\"text\"].apply(lambda x: cleaning(x))\n",
    "print(\"Cleaning Done\")\n",
    "\n",
    "news[\"for_glove\"]  = news[\"text\"].apply(lambda x: for_glove(x))\n",
    "print(\"For GloVe Done\")\n",
    "\n",
    "news[\"num_urls\"]= news[\"text\"].apply(lambda x: link_finder(x, False))\n",
    "print(\"Cleaning Done\")\n",
    "\n",
    "news[sentiment_values] = news[\"text\"].apply(sid.polarity_scores).apply(pd.Series)\n",
    "print(\"Sentiment Done\")\n",
    "\n",
    "news.to_csv(\"../EDA/cleaned/news.csv\")\n",
    "print(\"Saved to Directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this notebook, we have cleaned data and for visualizations, we will go to the next EDA notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
